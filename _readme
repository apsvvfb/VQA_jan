0. trainIDs.txt testIDs.txt(done)
[have to remove the gray images]
genIDs/generateIDs.sh
trainID: question_answers_jan.json + /home/c-nrong/VQA/VG_100K/
          (genIDs/trainID.txt) -- 64260 imgs
testID:  question_answers_jan.json + /home/c-nrong/VQA/VG_100K_2/
          (genIDs/testID.txt)  -- 39696 imgs

1. to generate Attention maps of english questions(done)
#i. /home/c-nrong/VQA/draw/_all.sh (each question takes 28 seconds to generate AttenMaps)
#{/home/c-nrong/VQA/HieCoAttenVQA/genAttenMap.lua}
#ImgIDs_ori.txt : 5582.   from Japanese website		(running)
#ImgIDs_all.txt : 108077. VG_100K.txt + VG_100K_2.txt	
#ImgIDs_rest.txt: 102495. (108077-5582)			(*)
#Max number of English questions for each Img = 8
#input:  trainIDs.txt/testIDs.txt + English questions(less than 8) + images 
#output: many txts (attenmaps). 
#the number of txt for each image equals to the number of English questions for this image.
ii. /home/c-nrong/VQA/HieCoAttenVQA2
input: question_answers_genome.json(used as eval set) + [trainID+testID](103956imgs) + images
output: data/vqa_data_prepro.json(ix_to_img_test) + 'AttenmapsAndImgidx.h5('attenmaps','imgidx')
trainset: VQA dataset
evalset:  visual genome dataset(english)
(a)generate vqa_raw_train.json() +  vqa_raw_test.json(95175imgs. why 95175 instead of 103956: because some imgs don't have English questions)
  cd /home/c-nrong/VQA/HieCoAttenVQA2/
  cd data/
  python vqa_preprocess.py --download 1 --split 1 
  rm vqa_raw_test.json
  python vqa_preprocess_Eval.py
(b)download cnn model
  cd image_model/
  python download_model.py --download 'VGG' 
(c)Generate Question Features (data/vqa_data_prepro.h5, data/vqa_data_prepro.json)
  #I modified the preppro_vqa.py to add 'ix_to_img_test' to data/vqa_data_prepro.json
  cd  prepro/
  python prepro_vqa.py --input_train_json ../data/vqa_raw_train.json --input_test_json ../data/vqa_raw_test.json --num_ans 1000
(d)generate image feature(vqa_data_img_vgg_train.h5)
  #we don't need to extract feature for training set,so I comment some lines of prepro_img_vgg.lua
  cd prepro/
  th prepro_img_vgg.lua -input_json ../data/vqa_data_prepro.json -image_root /home/c-nrong/VQA/ -cnn_proto ../image_model/VGG_ILSVRC_19_layers_deploy.prototxt -cnn_model ../image_model/VGG_ILSVRC_19_layers.caffemodel
(e)eval.lua(AttenmapsAndImgidx.h5)
  #I modified eval.lua and misc/DataLoaderDisk.lua
  rm AttenmapsAndImgidx.h5
  th eval.lua

#2. draw/AttenMapstxt2Hdf5_one.py
#input: txts which generated by "1.i" + trainIDs.txt + testIDs.txt
#output: AreaAll.hdf5 ( 2-d: num of trainIDs x (196+196) )

3. data/vqa_preprocess_Jan.py(done)
input:  question_answers_jan.json + trainIDs.txt/testIDs.txt
output: vqa_raw_train.json(64260imgs-514048ques)
        vqa_raw_test.json (39696imgs-317564ques)

4. prepro/prepro_vqa_Jan.py (extract question feature)(done)
#I changed the tokenizer to mecab, and the question number to 8(each image have 8 japanese questions)
python prepro_vqa_jan.py --input_train_json ../data/vqa_raw_train.json --input_test_json ../data/vqa_raw_test.json --num_ans 1000

x. prepro/prepro_img_vgg.lua (extract image feature)(done)
th prepro_img_vgg.lua -input_json ../data/vqa_data_prepro.json -image_root /home/c-nrong/VQA/ -cnn_proto /home/c-nrong/VQA/HieCoAttenVQA2/image_model/VGG_ILSVRC_19_layers_deploy.prototxt -cnn_model /home/c-nrong/VQA/HieCoAttenVQA2/image_model/VGG_ILSVRC_19_layers.caffemodel

#5. prepro/prepro_atten.py 
#input: txts which generated by "1.i" + data/vqa_data_prepro.json(it removes some questions/imgs when extracting question features. this json is used to get the imgID)
5. prepro/prepro_atten_new.py(done) 
input: 
-data/vqa_data_prepro.json(it removes some questions/imgs when extracting question features. this json is used to get the imgID)
-/home/c-nrong/VQA/HieCoAttenVQA2/AttenmapsAndImgidx.h5('attenmaps','imgidx')
-/home/c-nrong/VQA/HieCoAttenVQA2/data/vqa_data_prepro.json(ix_to_img_test)
output: data/vqa_data_area_train.h5 + data/vqa_data_area_test.h5

6. train.lua
th train.lua

7. Analysis/

---------------------------------

1. v1_all_1dividedby196
P=(1/196, 1/196, ..., 1/196, a1, a2, ..., a196) [1x392]
M=WP=(m1,m2,m3, ...,m196) (mi is a scalar)
V=(m1v1,m2v2,...,m196v196) [vi is a feature vector(512x1) for region i of image j.]

2. v1_all_1
P=(1, 1, ..., 1, a1, a2, ..., a196)  [1x392]
M=WP=(m1,m2,m3, ...,m196) (mi is a scalar)
V=(m1v1,m2v2,...,m196v196) [vi is a feature vector(512x1) for region i of image j.]

3.v2_onlyA
P=(a1, a2, ..., a196) [1x196]
M=P=(m1,m2,m3, ...,m196) (mi is a scalar)
V=(m1v1,m2v2,...,m196v196) [vi is a feature vector(512x1) for region i of image j.]

3.v2_addone
P=(a1, a2, ..., a196) [1x196]
M=P+1=(a1+1, a2+1, ..., a196+1)=(m1,m2,m3, ...,m196) (mi is a scalar)
V=(m1v1,m2v2,...,m196v196) [vi is a feature vector(512x1) for region i of image j.]
